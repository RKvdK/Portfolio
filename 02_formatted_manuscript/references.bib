
@article{cleave1995,
	title = {Evaluation of {Methods} for {Ecological} {Inference}},
	volume = {158},
	url = {https://ideas.repec.org//a/bla/jorssa/v158y1995i1p55-72.html},
	abstract = {In ecological inference one uses data which are aggregated by areal units to investigate the behaviour of the individuals comprising those units. Aggregated data are readily available in many fields and within a wide variety of data structures. In the structures considered, the aggregate data are characterized by the absence of available data in the internal cells of a cross‐classification. The aim of the ecological methods is to estimate the expected frequencies of such internal cells, which may be conditional on chosen covariates. Four methods of ecological inference are reviewed and their properties and appropriateness considered. These methods are then applied to data for which the internal cells are known and their performances compared.},
	language = {en},
	number = {1},
	urldate = {2025-09-22},
	journal = {Journal of the Royal Statistical Society Series A},
	author = {Cleave, N. and Brown, P. J. and Payne, C. D.},
	year = {1995},
	note = {Publisher: Royal Statistical Society},
	pages = {55--72},
	file = {Snapshot:files/20/v158y1995i1p55-72.html:text/html},
}

@article{lovelace2015,
	title = {Evaluating the {Performance} of {Iterative} {Proportional} {Fitting} for {Spatial} {Microsimulation}: {New} {Tests} for an {Established} {Technique}},
	volume = {18},
	issn = {1460-7425},
	shorttitle = {Evaluating the {Performance} of {Iterative} {Proportional} {Fitting} for {Spatial} {Microsimulation}},
	number = {2},
	journal = {Journal of Artificial Societies and Social Simulation},
	author = {Lovelace, Robin and Birkin, Mark and Ballas, Dimitris and van Leeuwen, Eveline},
	year = {2015},
	pages = {21},
	file = {text/html Attachment:files/21/21.html:text/html},
}

@article{bang2005,
	title = {Doubly robust estimation in missing data and causal inference models},
	volume = {61},
	issn = {0006-341X},
	doi = {10.1111/j.1541-0420.2005.00377.x},
	abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Bang, Heejung and Robins, James M.},
	month = dec,
	year = {2005},
	pmid = {16401269},
	keywords = {Antidepressive Agents, Cognitive Behavioral Therapy, Computer Simulation, Data Interpretation, Statistical, Depression, Humans, Longitudinal Studies, Models, Statistical, Multicenter Studies as Topic, Myocardial Infarction},
	pages = {962--973},
}

@article{groves2010,
	title = {Total {Survey} {Error}: {Past}, {Present}, and {Future}},
	volume = {74},
	issn = {0033-362X},
	shorttitle = {Total {Survey} {Error}},
	url = {https://doi.org/10.1093/poq/nfq065},
	doi = {10.1093/poq/nfq065},
	abstract = {“Total survey error” is a conceptual framework describing statistical error properties of sample survey statistics. Early in the history of sample surveys, it arose as a tool to focus on implications of various gaps between the conditions under which probability samples yielded unbiased estimates of finite population parameters and practical situations in implementing survey design. While the framework permits design-based estimates of various error components, many of the design burdens to produce those estimates are large, and in practice most surveys do not implement them. Further, the framework does not incorporate other, nonstatistical, dimensions of quality that are commonly utilized in evaluating statistical information. The importation of new modeling tools brings new promise to measuring total survey error components, but also new challenges. A lasting value of the total survey error framework is at the design stage of a survey, to attempt a balance of costs and various errors. Indeed, this framework is the central organizing structure of the field of survey methodology.},
	number = {5},
	urldate = {2025-09-22},
	journal = {Public Opinion Quarterly},
	author = {Groves, Robert M. and Lyberg, Lars},
	month = jan,
	year = {2010},
	pages = {849--879},
}

@article{biemer2010,
	title = {Total {Survey} {Error}: {Design}, {Implementation}, and {Evaluation}},
	volume = {74},
	issn = {0033-362X},
	shorttitle = {Total {Survey} {Error}},
	url = {https://doi.org/10.1093/poq/nfq058},
	doi = {10.1093/poq/nfq058},
	abstract = {The total survey error (TSE) paradigm provides a theoretical framework for optimizing surveys by maximizing data quality within budgetary constraints. In this article, the TSE paradigm is viewed as part of a much larger design strategy that seeks to optimize surveys by maximizing total survey quality; i.e., quality more broadly defined to include user-specified dimensions of quality. Survey methodology, viewed within this larger framework, alters our perspectives on the survey design, implementation, and evaluation. As an example, although a major objective of survey design is to maximize accuracy subject to costs and timeliness constraints, the survey budget must also accommodate additional objectives related to relevance, accessibility, interpretability, comparability, coherence, and completeness that are critical to a survey's “fitness for use.” The article considers how the total survey quality approach can be extended beyond survey design to include survey implementation and evaluation. In doing so, the “fitness for use” perspective is shown to influence decisions regarding how to reduce survey error during design implementation and what sources of error should be evaluated in order to assess the survey quality today and to prepare for the surveys of the future.},
	number = {5},
	urldate = {2025-09-22},
	journal = {Public Opinion Quarterly},
	author = {Biemer, Paul P.},
	month = jan,
	year = {2010},
	pages = {817--848},
	file = {Full Text:files/24/Biemer - 2010 - Total Survey Error Design, Implementation, and Evaluation.pdf:application/pdf;Snapshot:files/25/nfq058.html:text/html},
}

@article{luiten2020,
	title = {Survey {Nonresponse} {Trends} and {Fieldwork} {Effort} in the 21st {Century}: {Results} of an {International} {Study} across {Countries} and {Surveys}},
	volume = {36},
	issn = {0282-423X},
	shorttitle = {Survey {Nonresponse} {Trends} and {Fieldwork} {Effort} in the 21st {Century}},
	url = {https://www.scopus.com/pages/publications/85091457186},
	doi = {10.2478/jos-2020-0025},
	abstract = {For more than three decades, declining response rates have been of concern to both survey methodologists and practitioners. Still, international comparative studies have been scarce. In one of the first international trend analyses for the period 1980–1997, De Leeuw and De Heer (2002) describe that response rates declined over the years and that countries differed in response rates and nonresponse trends. In this article, we continued where De Leeuw and De Heer (2002) stopped, and present trend data for the next period 1998–2015 from National Statistical Institutes. When we looked at trends over time in this new data set, we found that response rates are still declining over the years. Furthermore, nonresponse trends do differ over countries, but not over surveys. Some countries show a steeper decline in response than others, but all types of surveys show the same downward trend. The differences in (non)response trends over countries can be partly explained by differences in survey design between the countries. Finally, for some countries cost indicators were available, these showed that costs increased over the years and are negatively correlated with noncontact rates.},
	number = {3},
	urldate = {2025-09-22},
	journal = {Journal of Official Statistics},
	author = {Luiten, A and Hox, J.J.C.M. and de Leeuw, E.D.},
	year = {2020},
	keywords = {costs, fieldwork, noncontact, refusal, Response trend, survey design},
	pages = {469--487},
	file = {Full Text:files/26/Luiten et al. - 2020 - Survey Nonresponse Trends and Fieldwork Effort in the 21st Century Results of an International Stud.pdf:application/pdf},
}

@article{delden2016,
	title = {Accuracy of {Mixed}-{Source} {Statistics} as {Affected} by {Classification} {Errors}},
	volume = {32},
	issn = {0282-423X},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.1515/jos-2016-0032},
	abstract = {Publications in official statistics are increasingly based on a combination of sources. Although combining data sources may result in nearly complete coverage of the target population, the outcomes are not error free. Estimating the effect of nonsampling errors on the accuracy of mixed-source statistics is crucial for decision making, but it is not straightforward. Here we simulate the effect of classification errors on the accuracy of turnover-level estimates in car-trade industries. We combine an audit sample, the dynamics in the business register, and expert knowledge to estimate a transition matrix of classification-error probabilities. Bias and variance of the turnover estimates caused by classification errors are estimated by a bootstrap resampling approach. In addition, we study the extent to which manual selective editing at micro level can improve the accuracy. Our analyses reveal which industries do not meet preset quality criteria. Surprisingly, more selective editing can result in less accurate estimates for specific industries, and a fixed allocation of editing effort over industries is more effective than an allocation in proportion with the accuracy and population size of each industry. We discuss how to develop a practical method that can be implemented in production to estimate the accuracy of register-based estimates.},
	language = {EN},
	number = {3},
	urldate = {2025-09-12},
	journal = {Journal of Official Statistics},
	author = {van Delden, Arnout and Scholtus, Sander and Burger, Joep},
	month = sep,
	year = {2016},
	note = {Publisher: SAGE Publications},
	pages = {619--642},
	file = {SAGE PDF Full Text:files/29/van Delden et al. - 2016 - Accuracy of Mixed-Source Statistics as Affected by Classification Errors.pdf:application/pdf},
}

@article{burger2015,
	title = {Sensitivity of {Mixed}-{Source} {Statistics} to {Classification} {Errors}},
	volume = {31},
	doi = {10.1515/JOS-2015-0029},
	abstract = {For policymakers and other users of official statistics, it is crucial to distinguish real differences underlying statistical outcomes from noise caused by various error sources in the statistical process. This has become more difficult as official statistics are increasingly based upon a mix of sources that typically do not involve probability sampling. In this article, we apply a resampling method to assess the sensitivity of mixed-source statistics to source-specific classification errors. Classification errors can be seen as coverage errors within a stratum. The method can be used to compare relative accuracies between strata and releases, it can assist in deciding how to optimally allocate resources in the statistical process, and it can be applied in evaluating potential estimators. A case study on short-term business statistics shows that bias occurs especially for those strata that deviate strongly from the mean value in other strata. It also suggests that shifting classification resources from small and medium-sized enterprises to large enterprises has virtually no net effect on accuracy, because the gain in precision is offset by the creation of bias. The resampling method can be extended to include other types of nonsampling error.},
	journal = {Journal of official statistics},
	author = {Burger, Joep and Delden, Arnout and Scholtus, Sander},
	month = sep,
	year = {2015},
	pages = {489--506},
	file = {Full Text PDF:files/31/Burger et al. - 2015 - Sensitivity of Mixed-Source Statistics to Classification Errors.pdf:application/pdf},
}

@article{donatiello2022,
	title = {The joint distribution of income and consumption in {Italy}: an in-depth analysis on statistical matching},
	shorttitle = {The joint distribution of income and consumption in {Italy}},
	doi = {10.1481/ISTATRIVISTASTATISTICAUFFICIALE_3.2022.03},
	abstract = {This article presents an application of statistical matching methods to integrate the EU Statistics on Income and Living Conditions and the Household Budget Survey with the aim of creating a synthetic dataset that can permit an in-depth multidimensional analysis of households' economic poverty in Italy. The work takes stock of previous experiences done at the Italian National Institute of Statistics-Istat and proposes a modification of a well-known approach to the statistical matching of data from complex sample surveys. The redesigned method permits to create a synthetic dataset that preserves the marginal distribution of both the target variables. The proposed method is more complex than simpler donor-imputation methods and permits taking into account the final survey weights. The higher complexity requires some additional checks when validating the results of the whole application. Preliminary results, presented in this paper, are quite promising also because the work benefits from an accurate ex ante harmonisation strategy of the reference surveys and on the collection of useful data for the application of statistical matching methods.},
	author = {Donatiello, Gabriella and Frattarola, Doriana and Spaziani, Mattia and D'Orazio, Marcello},
	month = dec,
	year = {2022},
	file = {Full Text PDF:files/33/Donatiello et al. - 2022 - The joint distribution of income and consumption in Italy an in-depth analysis on statistical match.pdf:application/pdf},
}

@book{dorazio2006,
	title = {Statistical {Matching}: {Theory} and {Practice}},
	isbn = {978-0-470-02354-9},
	shorttitle = {Statistical {Matching}},
	abstract = {There is more statistical data produced in today’s modern society than ever before. This data is analysed and cross-referenced for innumerable reasons. However, many data sets have no shared element and are harder to combine and therefore obtain any meaningful inference from. Statistical matching allows just that; it is the art of combining information from different sources (particularly sample surveys) that contain no common unit. In response to modern influxes of data, it is an area of rapidly growing interest and complexity. Statistical Matching: Theory and Practice introduces the basics of statistical matching, before going on to offer a detailed, up-to-date overview of the methods used and an examination of their practical applications.  Presents a unified framework for both theoretical and practical aspects of statistical matching. Provides a detailed description covering all the steps needed to perform statistical matching. Contains a critical overview of the available statistical matching methods. Discusses all the major issues in detail, such as the Conditional Independence Assumption and the assessment of uncertainty. Includes numerous examples and applications, enabling the reader to apply the methods in their own work. Features an appendix detailing algorithms written in the R language.  Statistical Matching: Theory and Practice presents a comprehensive exploration of an increasingly important area. Ideal for researchers in national statistics institutes and applied statisticians, it will also prove to be an invaluable text for scientists and researchers from all disciplines engaged in the multivariate analysis of data collected from different sources.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {D'Orazio, Marcello and Zio, Marco Di and Scanu, Mauro},
	month = mar,
	year = {2006},
	note = {Google-Books-ID: U75yCqrdXW4C},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@misc{wickham2023,
	title = {tidyverse: {Easily} {Install} and {Load} the '{Tidyverse}'},
	copyright = {MIT + file LICENSE},
	shorttitle = {tidyverse},
	url = {https://cran.r-project.org/web/packages/tidyverse/index.html},
	abstract = {The 'tidyverse' is a set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step. Learn more about the 'tidyverse' at {\textless}https://www.tidyverse.org{\textgreater}.},
	urldate = {2025-09-24},
	author = {Wickham, Hadley and RStudio},
	month = feb,
	year = {2023},
	keywords = {ChemPhys},
}

@misc{bruggen2016,
	type = {webpagina},
	title = {Establishing the accuracy of online panels for survey research},
	url = {https://www.cbs.nl/en-gb/background/2016/15/establishing-the-accuracy-of-online-panels-for-survey-research},
	abstract = {Establishing the accuracy of online panels for survey research},
	language = {en-GB},
	urldate = {2025-10-24},
	journal = {Statistics Netherlands},
	author = {Brüggen, E. and van den Brakel, J. and Krosnick, J.},
	month = apr,
	year = {2016},
	note = {Last Modified: 2016-04-11T14:15:00+02:00},
	file = {Snapshot:files/45/establishing-the-accuracy-of-online-panels-for-survey-research.html:text/html},
}

@misc{sojka2025,
	title = {Statistical {Matching} using a {Non}-{Probability} {Sample} as {Auxiliary} {Dataset}},
	author = {Sojka, B.L.},
	month = nov,
	year = {2025},
}

@article{rubin1976,
	title = {Inference and missing data},
	volume = {63},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/63.3.581},
	doi = {10.1093/biomet/63.3.581},
	abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
	number = {3},
	urldate = {2025-11-27},
	journal = {Biometrika},
	author = {Rubin, B., Donald},
	month = dec,
	year = {1976},
	pages = {581--592},
	file = {Snapshot:files/64/63.3.html:text/html},
}

@book{little2019,
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Statistical {Analysis} with {Missing} {Data}, {Third} {Edition}},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	isbn = {978-0-470-52679-8 978-1-119-48226-0},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119482260},
	language = {en},
	urldate = {2025-11-27},
	publisher = {Wiley},
	author = {Little, Roderick and Rubin, Donald},
	month = apr,
	year = {2019},
	doi = {10.1002/9781119482260},
}

@article{dolan2002,
	title = {Benchmarking optimization software with performance profiles},
	volume = {91},
	copyright = {http://www.springer.com/tdm},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s101070100263},
	doi = {10.1007/s101070100263},
	number = {2},
	urldate = {2025-11-17},
	journal = {Mathematical Programming},
	author = {Dolan, Elizabeth D. and Moré, Jorge J.},
	month = jan,
	year = {2002},
	pages = {201--213},
}

@article{welford1962,
	title = {Note on a {Method} for {Calculating} {Corrected} {Sums} of {Squares} and {Products}},
	volume = {4},
	issn = {0040-1706, 1537-2723},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490022},
	doi = {10.1080/00401706.1962.10490022},
	abstract = {In many problems the "corrected sum of squares" of a set of values must be calculated i.e. the sum of squares of the deviations of the values about their mean. The most usual way is to calculate the sum of squares of the values (the "crude" sum of squares) and then to subtract a correction factor (which is the product of the total of the values and the mean of the values). This subtraction results in a loss of significant figures and if a large set of values is being handled by a computer, this can result in a corrected sum of squares which has many fewer, accurate significant figures than the computer uses in calculations. Various alternative schemes are available to combat this. One method is to scale the values to an arbitrary origin which is approximately equal to the mean: if successful, this will reduce the loss in significant figures. An alternative method is to first calculate the mean and then sum the powers of the deviations from the mean. This involves each value being considered twice: first in evaluating the mean and then when calculating its deviation from the mean. If the set of values is large and is being handled by a computer this can involve either storing the data in a slow speed store or reading the same data into the computer twice. A third method which is less cumbersome than either of these is outlined below. The basis of the method is an iteration formula for deriving the corrected sum of squares for n values from the corrected sum of squares for the first (n 1) of these. We are given a set of xi's (i = 1, * *, k,) for which we require the corrected sum of squares.},
	language = {en},
	number = {3},
	urldate = {2025-11-17},
	journal = {Technometrics},
	author = {Welford, B. P.},
	month = aug,
	year = {1962},
	pages = {419--420},
}

@book{waal2015,
	address = {Den Haag},
	title = {Statistical matching: {Experimental} results and future research questions},
	shorttitle = {Statistical matching},
	abstract = {National statistical institutes try to construct data sets that are rich in information content by combining available data as much as possible. Unfortunately, units, e.g. persons or enterprises, in different data sources cannot always be matched directly with each other, for example because different data sources often contain different units. In such a case one can sometimes resort to statistical matching rather than exact matching. Statistical matching can be used when different data sources contain (different) units with a set of common (background) variables. These common variables may then be used to match similar units in the data sources to each other. From March 2015 till the end of June 2015 two master students, Sofie Linskens and Janneke van Roij, at Tilburg University evaluated some methods for statistical matching, namely random hot deck, distance hot deck and statistical matching using a parametric model, on categorical data from the Dutch Population Census 2001. In this paper we describe the methods that they examined and the results they obtained.},
	publisher = {CBS},
	author = {de Waal, A.G.},
	year = {2015},
}

@misc{wickham2025,
	title = {ggplot2: {Create} {Elegant} {Data} {Visualisations} {Using} the {Grammar} of {Graphics}},
	copyright = {MIT + file LICENSE},
	shorttitle = {ggplot2},
	url = {https://cran.r-project.org/web/packages/ggplot2/index.html},
	abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
	urldate = {2025-12-18},
	author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey and Brand, Teun van den and Posit and PBC},
	month = nov,
	year = {2025},
	keywords = {ChemPhys, NetworkAnalysis, Phylogenetics, Spatial, TeachingStatistics},
}

@misc{muller2025,
	title = {here: {A} {Simpler} {Way} to {Find} {Your} {Files}},
	copyright = {MIT + file LICENSE},
	shorttitle = {here},
	url = {https://cran.r-project.org/web/packages/here/index.html},
	abstract = {Constructs paths to your project's files. Declare the relative path of a file within your project with 'i\_am()'. Use the 'here()' function as a drop-in replacement for 'file.path()', it will always locate the files relative to your project root.},
	urldate = {2025-12-18},
	author = {Müller, Kirill and Bryan, Jennifer},
	month = sep,
	year = {2025},
	keywords = {ReproducibleResearch},
}
